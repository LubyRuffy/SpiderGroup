SpiderGroup 站群抓取服务
=============

###### 目标用户：
建立了站群，有不同的网站类型，需要每天自动更新内容。缺乏技术和时间。需要请人打理。

###### 预期目标：
用户可以选择抓取的来源网站（网站类型），可以自定义入库内容的标题关键字。

###### 后台的功能需求：
1. 能够按网站定期入库（默认1天，可以每小时）
2. 如果来源网站没有内容入库需要触发告警
3. 自动去重，保留原始日期
4. 提供API接口？

###### 流程：
* 获取初始url内容（配置初始URL）
* 根据规则提取标题、url、概要、封面图片、作者等信息（配置内容页URL提取规则，每个不一样，最好封装成方法）
* 获取内容页内容，提取详细信息入库（配置内容页内容的提取规则，有可能有分页，最好封装成方法）

###### 一些细则要求：
* 可以根据URL，标题关键字，正文关键字等信息设置filter（正则）
* 可以设置替换的正则表达式数组（比如freebuf后面会加入一段版权的话需要去除）
* 增加图片链接保存替换功能(可以保存本地或者网络云存储)
* 基本信息包括：标题，url，正文，作者，发布时间，概要，封面图片
* 可以提取异步加载的图片（比如制定data-original，data-src等)
* 可以提取img src中数据为base64格式的图片
* 一小时内不再抓取
* 支持网络代理

###### 运行：
1. cd website
2. bundle install
3. 新建cron任务，crontab -e加入执行，建议每30分钟执行一次daemon.sh
  如果不需要定期执行，也可以手动执行./spidergroup all
4. unicorn_rails 运行看效果


###### 网站功能介绍：
1. 入库文章在published置1后显示(ok)
2. 支持多用户登陆(用户名，头像)
3. 支持编辑器(ok)
4. 用户支持权限等级
5. 支持收藏(ok)
6. 支持分享转发(ok)
7. 支持赞(ok)
8. 支持审核
9. 微博，qq等形式登陆